---
title: "Machine Learning Class Project"
author: "Heather Chen-Mayer"
date: '`r format(Sys.time(), "%B %d, %Y")`'
output:
html_document: default
pdf_document: default
word_document: default
---

## Executive Summary:
This class project uses the Weight Lifting Exercise Dataset (http://groupware.les.inf.puc-rio.br/har) collected from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. A training dataset is used to create a model, and to estimate the out-of-sample error of the model, a 10-fold cross validation is performed using the training data.  Two models were tested, one from the "caret" package using the "knn" method, the other from the "rpart" package with a generic classification method. The two yielded vary different accuracies, as well as the 95% confidence interval representing the out-of-sample error. They are 0.75 [0.70, 0.79] and 0.40 [0.24, 0.55], respectively.  Not too surprisingly, when applied to the testing data, the predictions from the two models agree only 5 out 20 trials. Such disparity raises caution in modeling in general. Since the testing data set has no ground truth, it is difficult to assess the validity of the models.  


## 1. Getting and cleaning up data, with thse steps:
Remove NA's from the training data rows.    
Data columns: Not all variables in the testing data have values.  Adjust training data accordingly, i.e. delete the variables from the training data that have no entry in the testing data.     
There are 160 variables in the training and testing data, not all are important. Use reprocessing with principle components to reduce the number of variables (down to 14).


```{r,echo=T}


library(caret)

library(rpart)

setwd("C:/Users/chenmaye/Documents")
p.training<-read.csv("pml-training.csv") #19622 rows, 160 columns
p.testing<-read.csv("pml-testing.csv")

#clean up

message("training dataset dimension")
dim(p.training)
#p.training1<-na.omit(p.training) #down to 406 rows, but still 160 columns. The last column is classe. too trastic in cutting out entire rows
p.training1r<-p.training[  ! apply( p.training , 1 , function(x) all(is.na(x)) ) ,] #remove entire NA rows, no change (still 19622 x 160)
p.training1<-p.training1r[,  ! apply( p.training1r , 2 , function(x) all(is.na(x)) ) ] #remove entire NA columns, no change (still 19622 x 160)
message("training dataset dimension after clean up")
dim(p.training1)

message("testing dataset dimension")
dim(p.testing)
#p.testing1<-na.omit(p.testing) #this omits rows
p.testing1r<-p.testing[  ! apply( p.testing , 1, function(x) all(is.na(x)) ) ,] #remove entire NA row, down to 60 columns
p.testing1<-p.testing1r[ , ! apply( p.testing1r , 2 , function(x) all(is.na(x)) ) ] #remove entire NA columns, down to 60 columns
message("testing dataset dimension after clean up")
dim(p.testing1)

p.training2<-p.training1[,names(p.training1) %in% names(p.testing1)] #keep only the same columns in the testing data, down to 59 columns

preProc<-preProcess(p.training2,method="pca",thresh=0.8) #the last column is classe

p.training2PC<-predict(preProc,p.training2) #generate PC for training data, down to 14 variables
dim(p.training2PC)
#[1] 19622    17
p.training2PC.use<-p.training2PC[,grepl("PC",names(p.training2PC))] #cut out the non "PCx" columns
dim(p.training2PC.use)
#[1] 19622    14
```
## 2. Working with training data 
Two functions are used to explore the training and cross validation to estimate the out-of-sample error:   
*"train" function with "knn" method from package "caret"   
*"rpart" function with "class" method from package "rpart"  

### 2.1 use "train" function, without k-fold cross validation:
By default, knn method uses Bootstrapping method to set k nearest neighbors, and it's unfortunate that the k is used to represent that --  confused with k-fold CV where data is divided into k folds --  both 2.1 and 2.3 (below) yielded k=5 being the final model, when they represent two different things. Or do they?? The outcome tables are identical.

```{r,echo=T}
#traning
set.seed(123) 
message("single fold (not breaking up the training set to k-fold)")
modelFit <- train(x = p.training2PC.use, y = p.training1$classe,method = "knn") 
training2.pred<-predict(modelFit,p.training2PC.use)
message("Accuracy:")
confusionMatrix(p.training1$classe,training2.pred)[[3]]
message("original data:")
table(p.training2PC$user_name,p.training1$classe) #original data
message("model prediction rpart")
table(p.training2PC$user_name,training2.pred) #outcome from the single model prediction with method "class"
```
### 2.2 "train" function, still without cross validation, treating the object as a "formula class". This should be identical to the above, just trying to understand the semantics. 

```{r,echo=T}

set.seed(123) 
message("the above can also be used in the formula form:")
p.training2PC.use.for<-data.frame("classe"=p.training1$classe,p.training2PC.use) #put in the same data frame so that the formula form can be used
modelFit.for <- train(classe~., data = p.training2PC.use.for, method = "knn") 
message("this is a formula class, it doesn't accept trControl, so no effect on the k-fold.")
training2.for.pred<-predict(modelFit.for,p.training2PC.use.for)
message("Accuracy:")
confusionMatrix(p.training1$classe,training2.for.pred)[[3]]
message("#outcome from the single model prediction with method class")
table(p.training2PC$user_name,training2.for.pred) 

```

### 2.3 Training with "train" function, with 10-fold cross validation.

```{r, echo=T}
# names(getModelInfo()) to see a list of methods
#k-fold=10
# Define training control
set.seed(123) 
train.control <- trainControl(method = "cv", number = 10) #set up for 10 fold cross validation
# Train the model


modelFitK <- train(x = p.training2PC.use, y = p.training1$classe,method = "knn",trControl = train.control)
training2K.pred<-predict(object=modelFitK, newdata=p.training2PC.use)
message("Accuracy:")
confusionMatrix(training2K.pred,p.training1$classe)[[3]] # rpart can do this explicitly 
# note: formula form doesn't work with trControl!  So can't do k-fold in formula form.
message("outcome from knn model with 10-fold cv:")
table(p.training2PC$user_name,training2K.pred) 


```


### 2.4 Training with "rpart" function, without cross validation.
This is really fast compared to the knn method. But the accuracy is only 0.6. 

```{r,echo=T}



#back up a step
# assess training data all at once
#http://t-redactyl.io/blog/2015/10/using-k-fold-cross-validation-to-estimate-out-of-sample-accuracy.html
set.seed(123) 
model.single<-rpart(classe ~ ., data=p.training2PC.use.for,method="class")

predict.single<-predict(object=model.single,newdata=p.training2PC.use.for, type="class")
message("Accuracy:")
confusionMatrix(predict.single, p.training2PC.use.for$classe)[[3]] #ok this makes more sense, accuracy of 0.64.


message("outcome from the single model prediction with method class")
table(p.training2PC$user_name,predict.single) 
```
### 2.5 Training with "rpart" function with 10-fold cross validation, as well as with repeat 10-fold cv, which reports a 95% confidence interval for the estimated accuracy.  
```{r, echo=T}

#Estimating out-of-sample accuracy 
#k-fold cross validation
set.seed(123) 

k.folds <- function(k) {
  folds <- createFolds(p.training2PC.use.for$classe, k = k, list = TRUE, returnTrain = TRUE)
  for (i in 1:k) {
    model <- rpart(classe ~ ., 
                   data = p.training2PC.use.for[folds[[i]],], method = "class")
    predictions <- predict(object = model, newdata = p.training2PC.use.for[-folds[[i]],], type = "class")
    accuracies.dt <- c(accuracies.dt, 
                       confusionMatrix(predictions, p.training2PC.use.for[-folds[[i]], ]$classe)$overall[[1]])
  }
  accuracies.dt
}

set.seed(123)
accuracies.dt <- c()
accuracies.dt <- k.folds(10)
message("Accuracies from each fold:")
accuracies.dt
message("Mean Accuracy:")
mean(accuracies.dt)

## repeat k-fold
set.seed(123)
v <- c()
v <- replicate(10, k.folds(10)) #this takes a long time. Reduce the replicate to 10 just to get an idea.
accuracies.dt <- c()
for (i in 1 : 10) { 
  accuracies.dt <- c(accuracies.dt, v[,i])
}

mean.accuracies <- mean(accuracies.dt)
lci <- mean(accuracies.dt) - sd(accuracies.dt) * 1.96
uci <- mean(accuracies.dt) + sd(accuracies.dt) * 1.96
message("histogram of accuracies using repeat 10-fold CV:")
hist(accuracies.dt,freq=F)

message("Mean Accuracy, Lower CI, Upper CI")
c(mean.accuracies, lci, uci)

```
## 3. Testing data 
There is no "ground truth" in the testing data set, and therefore this is strictly for prediction.  The outcomes from the two training models created from knn in "caret" and from "rpart" are compared.The two models agreed 14 out of 20 trials. Considering the disparity in accurary (0.96 vs 0.60), this disagreement is not surprising.  Of course the knn results are used for reporting.
```{r,echo=T}
## testing ##

p.testing1PC<-predict(preProc,p.testing1[,-60]) #general PCA from test data

p.testing1PC.use<-p.testing1PC[,grepl("PC",names(p.testing1PC))] #cut out the non "PCx" columns


#testing model from caret-knn
testing1.pred<-predict(modelFit,p.testing1PC) 
message("Testing result using model from caret-knn")
table(p.testing1PC$user_name,testing1.pred)


# testing1K.pred <- modelFitK %>% predict(p.testing1PC) #this is another way of coding predictions

#testing model from rpart-class
testing.rpart.single.pred<- predict(object = model.single, newdata = p.testing1PC.use, type = "class")
message("testing result from an rpart model:")
table(p.testing1PC$user_name,testing.rpart.single.pred)

test.df<-data.frame("user_name"=p.testing1PC$user_name,"knn pred"=testing1.pred,"rpart pred"=testing.rpart.single.pred)

message("Comparison of the results of testing using the two models:")
test.df


```
## 4. Remaining question: 
The paper referenced in the original data set  
http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf  
stated "Because of the characteristic noise in the sensor data, we used a Random Forest approach..." So it
looks like I should be using the Random Forest model after all, since the answers to the quiz were 80% correct based on the knn model. However, it takes too long!  Here is the code:

```{r, echo=T}

## 
##   
## set.seed(123) 
## # Train the model
## 
## modelFitrf <- train(x = p.training2PC.use, y = p.training1$classe,method = "rf")
## training2rf.pred<-predict(object=modelFitrf, newdata=p.training2PC.use)
## 
## train.control <- trainControl(method = "cv", number = 2,verboseIter=F) #set up for 2 fold cross validation
## modelFitrf.cv <- train(x = p.training2PC.use, y = p.training1$classe,method = "rf",trControl = ## train.control)
## 
## training2rf.pred<-predict(object=modelFitrf, newdata=p.training2PC.use)
## message("Accuracy:")
## confusionMatrix(training2rf.pred,p.training1$classe)[[3]] 
## message("outcome from RF model with 5-fold cv:")
## table(p.training2PC$user_name,training2rf.pred) 
## 
## testing1.rf.pred<-predict(modelFitrf,p.testing1PC) 
## message("Testing result using model from caret-rf")
## table(p.testing1PC$user_name,testing1.rf.pred)
## 
## test.df<-data.frame("user_name"=p.testing1PC$user_name,"knn pred"=testing1.pred,"rpart ## pred"=testing.rpart.single.pred,"rf pred"=testing1.rf.pred)
## 
## message("Comparison of the results of testing using the three models:")
## test.df

```
